{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data Loaded. The data size after flipping is\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Lambda, Convolution2D, Cropping2D, Dropout\n",
    "\n",
    "print('Loading data...')\n",
    "lines = []\n",
    "with open('K:/AI/SDCND/term1/CarSimGitHub/CarND-Behavioral-Cloning-P3-master/windows_sim/data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        lines.append(line)\n",
    "\n",
    "images =[]\n",
    "measurements = []\n",
    "\n",
    "#PreProcessing\n",
    "#Upon inspecting the recorded data, we found that an extremely high number of recorded steering values were 0.0 as we ideally\n",
    "#want to drive in the middle of road\n",
    "#Hence we look out for images corresponding to that 0.0 steering angle, and choose to select all values except for the 0 values\n",
    "#So when the model drives, it doesn't converge at 0 as the most optimal output due to overpopulation of 0\n",
    "\n",
    "Offset = 1.2\n",
    "\n",
    "for x in lines:\n",
    "    if (x[3]!=\"0\"):\n",
    "        image_path=x[0]\n",
    "        image=cv2.imread(image_path,cv2.IMREAD_COLOR)\n",
    "        images.append(image)\n",
    "        measurements.append((float(x[3])*Offset))       \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "X_train = np.array(images)\n",
    "Y_train = np.array(measurements)\n",
    "\n",
    "augmented_images, augmented_measurements = [], []\n",
    "for image, measurement in zip(images,measurements):\n",
    "    augmented_images.append(image)\n",
    "    augmented_measurements.append(measurement)\n",
    "    augmented_images.append(cv2.flip(image,1))\n",
    "    augmented_measurements.append(measurement*(-1.0))\n",
    "\n",
    "X_train = np.array(augmented_images)\n",
    "Y_train = np.array(augmented_measurements)\n",
    "\n",
    "print('Data Loaded. The data size after flipping is')\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(input):\n",
    "    from keras.backend import tf as ktf\n",
    "    return ktf.image.resize_images(input, (60, 140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 386 samples, validate on 4 samples\n",
      "Epoch 1/5\n",
      "386/386 [==============================] - 20s - loss: 0.1367 - val_loss: 0.0246\n",
      "Epoch 2/5\n",
      "386/386 [==============================] - 15s - loss: 0.0618 - val_loss: 0.0212\n",
      "Epoch 3/5\n",
      "386/386 [==============================] - 15s - loss: 0.0369 - val_loss: 0.0165\n",
      "Epoch 4/5\n",
      "386/386 [==============================] - 15s - loss: 0.0320 - val_loss: 0.0125\n",
      "Epoch 5/5\n",
      "386/386 [==============================] - 15s - loss: 0.0262 - val_loss: 0.0131\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_10 (Lambda)               (None, 160, 320, 3)   0           lambda_input_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_6 (Cropping2D)        (None, 79, 180, 3)    0           lambda_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)               (None, 60, 140, 3)    0           cropping2d_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (None, 56, 68, 24)    1824        lambda_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (None, 26, 32, 36)    21636       convolution2d_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (None, 22, 28, 48)    43248       convolution2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (None, 10, 13, 64)    27712       convolution2d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_25 (Convolution2D) (None, 8, 11, 64)     36928       convolution2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 5632)          0           convolution2d_25[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 1164)          6556812     flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 100)           116500      dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 50)            5050        dense_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 10)            510         dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 1)             11          dense_24[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 6,810,231\n",
      "Trainable params: 6,810,231\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Lambda,Convolution2D, Cropping2D, Dropout\n",
    "model=Sequential()\n",
    "\n",
    "\n",
    "\n",
    "#Preprocessing:\n",
    "#Changing mean and standard deviation so that the model scales properly. We multiply it by a contrasting factor to \n",
    "# increase the contrast and thus increase the deviation\n",
    "                                   \n",
    "model.add(Lambda(lambda x: ((x / 255.0) - 0.5), input_shape=(160,320,3)))\n",
    "#Cropping the unnecessary parts like : sky, trees, hood of car, ground off the road\n",
    "model.add(Cropping2D(cropping=((60,21),(70,70))))\n",
    "\n",
    "# re-size inside the model\n",
    "model.add(Lambda(resize_img))\n",
    "\n",
    "#NVidia Model\n",
    "#Find more about this at https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/\n",
    "model.add(Convolution2D(24,5,5,subsample=(1,2),activation='relu'))\n",
    "model.add(Convolution2D(36,5,5,subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(48,5,5,activation='relu'))\n",
    "model.add(Convolution2D(64,3,3,subsample=(2,2),activation='relu'))\n",
    "model.add(Convolution2D(64,3,3,activation='relu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1164))\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(X_train,Y_train,validation_split=0.01,batch_size = 64,shuffle=True,nb_epoch=5)\n",
    "model.summary()\n",
    "model.save('model.h5')\n",
    "\n",
    "#first i Travelled twice clockwise and twice anticlockwise\n",
    "#However that data was insufficient to handle the sharp curves and bridge\n",
    "#I noted where the car steered towards on these curves and bridge, went in to training mode and recorded me correcting my\n",
    "#path from these wrong positions the car previously steered towards. This allowed the model to learn about what to do\n",
    "#when it messes up its steering at odd paths, the result with this was : car offshooted at the right curve\n",
    "\n",
    "#Next I plan on experimenting more with the acitvations functions, right now i have a relu at all the convolutions\n",
    "#and one relu at the first fully connected layer. This time I introduce elu instead. Results : disastrous, the car drove off \n",
    "#the road immediately\n",
    "\n",
    "#I switch back the relus and instead add a tanh for the first fully connected layer, car smashes at diverting point after bridge\n",
    "\n",
    "#No activations in the FCCs, result : the car blocks on bridge and right turn\n",
    "\n",
    "#Recapturing data\n",
    "\n",
    "\n",
    "#increase the size of validation set to 3%\n",
    "#and Try removing dropout, result : overshoots on first left turn\n",
    "\n",
    "#dropout added for all layers, result is bad. The output is frozen at -1.29\n",
    "\n",
    "#dropout only to be for convolution layers, car again overshoots. Get rid of dropouts over all.\n",
    "\n",
    "#all dropouts removed, first left turn specifically trained. Results : much better, stops at the first left.\n",
    "\n",
    "#Removing all dropouts, much better results but the car blocks at the first left\n",
    "\n",
    "#Removing all relus from convolutions as well, not good at all. The car immediately goes off the road\n",
    "\n",
    "#Retraining the first left turn, and relus added back. Result : blocks at bridge\n",
    "\n",
    "#retraining the entire loop with left>right>left>right instead of using mouse. Trained only one loop in counter clockwise,\n",
    "#results : first left wasn't clean but the car crossed it and it stuck on the next left turn\n",
    "\n",
    "#retrained it for the second left turn. Results : in general the car sticks to the left\n",
    "\n",
    "#retraining the entire set again. One turn clockwise and one anti clockwise. Results : not good\n",
    "\n",
    "\n",
    "#retrained with one loop. Removed steering noise, removed contrast enhancement, removed subsample of 2x2 from first Conv layers\n",
    "#result : the computer hanged, maybe because of the 2x2 subsample. Let's add it back to alternate layers. Result :\n",
    "#overshoots at first left\n",
    "\n",
    "#restore the previous data loading and segmenting method; three if loops. See if the result is similar, Result : Great results\n",
    "#the first track was completed with a little errors in turns. The car fared surprisingly well on the tougher track too\n",
    "\n",
    "#trained a few more cases of sharp curves, and also added the 2x contrasting multiplier, epochs = 10. Result : blocked at bridge\n",
    "\n",
    "#Remove the 2x multiplier, epochs = 5. Result : not good.\n",
    "\n",
    "#So we record all at once, once again . Result : Things were pretty smooth until the first left.\n",
    "\n",
    "#We retrain that particular bit. Result is worse, I think we should record all the data in one go. now\n",
    "\n",
    "#recording everything in one go, pressed keys for as short duration as possible and tapped them quickly,\n",
    "#result : HUUUGE dataset of 18k, result : Not any good, had issues with going straight and also while taking right turns.\n",
    "\n",
    "#Now I try to train it on the tougher track, try to allign it to the middle lane line as that is a contrasting area which the network\n",
    "#can learn to follow. Result on the tougher track :  COMMENDABLE RESULTS!!! the whole track was completed, though there were \n",
    "#some overshoots at the sharpest turns of the track(which can be corrected by specefically training them\n",
    "#, but overall it was pretty smooth. Result on the easier track : Overshoots on the second left. I've observed that it has\n",
    "#a tendency to drive over the lane, probably because it learned to do that from the center lane marking from the tougher track.\n",
    "\n",
    "#I'll train the sharp curves and also train the model to recover from it's excessive right orientation on the road and some dropout\n",
    "#result : the output was frozen, definitely because of dropout. Have to retrain without it now\n",
    "\n",
    "#without dropout, result : it doesn't perform that good on the easy track, but it's quite good on the tougher one. I retrained\n",
    "#the slight overshoots further\n",
    "\n",
    "#right now I just want to create a model for the tough track since it doesn't seem so tough given my recent progress\n",
    "#result after retraining the overshoots : First overshoot is absolutely corrected! The car overshoots at a cliff\n",
    "\n",
    "#retraining a new model for easier track. For training i do wiggly zig zag driving for only half the track, result : \n",
    "#performed quite good for its dataset size haha. \n",
    "\n",
    "#let's try the wiggly motion approach for the entire track. Result : did quite good, overshot at second left but that's correctable\n",
    "\n",
    "#correcting the second left overshot, results : it's going a little too much on the left in general case\n",
    "\n",
    "#added a 2x contrasting multiplier, results : the wiggling was enhanced\n",
    "\n",
    "#thus i should train it using stable driving and then add a multiplier, results : it drove out on the left\n",
    "#training a round in opposite direction result : quite good, just a little allignement issues \n",
    "\n",
    "#retrained those allighnement problem regions, result : overshot after the first left. \n",
    "\n",
    "#try with removing the 2x multiplier, added a 1.5 multiplier to incoming data and 1.1 multiplier to recorded steering measurments\n",
    "#result : very good! though there's a slight offset to the right which can be trained.\n",
    "\n",
    "#after offset training : results : works like a charm, minor block at pre bridge and at bridge. Let's reduce contrast multiplier\n",
    "#and see how it goes without training anything extra\n",
    "\n",
    "#try removing the contrast multiplier : results :  went out, seems like contrast multiplier enhances the sensitivity, and that felt like it was missing here\n",
    "#Val_loss = 0.087\n",
    "\n",
    "#we observe that the validation loss is always about double the training loss, Hence we have an overfitting model. \n",
    "#Therefore we increase epochs(second), increase validation set(first), add regularization(first)\n",
    "#Val_loss =   0.1391     Result = There's a strong similarity in the wavy driving pattern. Overshoots on the first left.\n",
    "\n",
    "#we play with contrast and offset multipliers. Remove the offset because that mismatches the training data.\n",
    "#we add contrast of 1.2 and add regularization to all the layers\n",
    "#Val_loss = 0.5      Result = Training is quite good, val_loss is only slightly above training loss. Track result : output froze\n",
    "#to a single steering angle, just like it did with dropout\n",
    "\n",
    "#turns out we can't really stop overfitting, we might use softer values but otherwise things might just freeze up\n",
    "\n",
    "#remove all regularization. Used a 1.2x contrast.2x Steering Result : Things are shaky but it has survived all of it. Can probably\n",
    "#retrain with smoother data and then use a 2x multiplier. Saving this model as Model_s2_c1.2\n",
    "\n",
    "#Turns out that we don't really need to worry about losses for this project, as shows on multiple occasions, trying to reduce\n",
    "#the overfit only leads to a frozen fixed steering angle.\n",
    "\n",
    "#Retraining with smoother data. 1x contrast 1.5x steering. Great results. There's a lack of bridge data as we just went straight\n",
    "#on it, adding some data for the bridge\n",
    "\n",
    "#results : contrast multiplier decreases results. We might even change it to something less than one to improve results.\n",
    "\n",
    "#added back image flipping to further enhance the model, Steering multiplier is 1.5x as the dataset is big now so the model knows\n",
    "#most of its way around the track. Result : is a little wavy here and there but it gets the job done! \n",
    "#there's clearly more room for improvement.\n",
    "\n",
    "#if things are same or better, try increasing the epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
